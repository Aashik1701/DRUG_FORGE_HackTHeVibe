# ðŸ§ª backendML â€” Model Research & Training

This folder contains **research assets, training notebooks, and model development code** for the AI models powering DrugForge.

---

## ðŸŽ¯ Quick Answer: Which Backend Should I Use?

| Use Case | Folder | Command |
|----------|--------|---------|
| **ðŸš€ Running the live API** | `../backend/` | `uvicorn main:app --reload` |
| **ðŸ”¬ Training/retraining models** | `backendML/` | `python train_all_models.py` |
| **ðŸ“š Exploring model notebooks** | `backendML/` | Jupyter notebooks (see below) |
| **ðŸ‘¨â€ðŸ’» Contributing new models** | `backendML/` | Add to `train_all_models.py` |

---

## ðŸ“ Folder Structure

```
backendML/
â”‚
â”œâ”€â”€ ðŸ“„ README.md                    â† You are here
â”œâ”€â”€ ðŸ“„ requirements.txt             â† Model training dependencies
â”œâ”€â”€ ðŸ app.py                       â† Legacy Flask API (reference only)
â”œâ”€â”€ ðŸ”¨ train_all_models.py          â† MAIN: Train all 9 models at once
â”‚
â”œâ”€â”€ ðŸ“Š ADMET Properties/            â† Absorption, Distribution, Metabolism, Excretion, Toxicity
â”‚   â”œâ”€â”€ Datasets/
â”‚   â”‚   â”œâ”€â”€ bbb_martins.tab         â† Blood-Brain Barrier penetration data
â”‚   â”‚   â”œâ”€â”€ cyp3a4_veith.csv        â† CYP3A4 inhibition data
â”‚   â”‚   â”œâ”€â”€ half_life_obach.csv     â† Drug elimination half-life data
â”‚   â”‚   â”œâ”€â”€ herg_karim.tab          â† Heart toxicity (hERG) data
â”‚   â”‚   â””â”€â”€ solubility_aqsoldb.tab  â† Water solubility data
â”‚   â””â”€â”€ Notebook/
â”‚       â”œâ”€â”€ BBBP.ipynb              â† Train BBB model
â”‚       â”œâ”€â”€ CYP P450 3A4 Inhibition.ipynb â† Train CYP3A4 model
â”‚       â”œâ”€â”€ Solubility.ipynb        â† Train solubility model
â”‚       â”œâ”€â”€ Toxicity.ipynb          â† Train toxicity models (HEPG2, BBBP-Tox)
â”‚       â””â”€â”€ Excretion.ipynb         â† Train half-life model
â”‚
â”œâ”€â”€ ðŸŽ¯ Drug Target Binding Score/   â† Protein-ligand binding affinity
â”‚   â”œâ”€â”€ Binding_Score.ipynb         â† Train binding prediction model
â”‚   â””â”€â”€ Datasets/                   â† (Binding training data)
â”‚
â”œâ”€â”€ ðŸ§¬ Molecular Docking/           â† 3D structure generation & docking
â”‚   â”œâ”€â”€ molecular_docking.ipynb     â† Reference: AutoDock workflow
â”‚   â”œâ”€â”€ sample.smi                  â† Example SMILES strings
â”‚   â””â”€â”€ *.mol2                      â† Example docked structures
â”‚
â””â”€â”€ ðŸŽª Target Identification/       â† Drug target prediction
    â”œâ”€â”€ Datasets/
    â”‚   â”œâ”€â”€ ACE-2.csv               â† ACE2 receptor binding data
    â”‚   â””â”€â”€ COX-2.csv               â† COX-2 enzyme binding data
    â””â”€â”€ Notebook/
        â”œâ”€â”€ ace2.ipynb              â† Train ACE2 binding model
        â”œâ”€â”€ cox2.ipynb              â† Train COX-2 binding model
        â””â”€â”€ hepg2.ipynb             â† Train HepG2 toxicity model
```

---

## ðŸš€ How to Train Models

### Option 1: Train All Models at Once (Recommended)

```bash
cd backendML
python3 -m venv venv
source venv/bin/activate                    # Windows: venv\Scripts\activate
pip install -r requirements.txt
python train_all_models.py
```

**Output:** Trained `.pkl` files saved to `../backend/models/`

**Time:** ~5-10 minutes (depends on dataset sizes)

### Option 2: Train Individual Models (Jupyter Notebooks)

```bash
cd backendML
jupyter notebook
```

Then open any notebook in `ADMET Properties/Notebook/` or `Target Identification/Notebook/`

**Why:** Experiment with hyperparameters, visualize training curves, debug data

---

## ðŸ“Š Datasets Reference

| Dataset | File | Rows | Task | Model |
|---------|------|------|------|-------|
| BBB Penetration | `bbb_martins.tab` | 2,000+ | Binary classification | BBBP|
| CYP3A4 Inhibition | `cyp3a4_veith.csv` | 13,000+ | Binary classification | CYP3A4 |
| Water Solubility | `solubility_aqsoldb.tab` | 9,000+ | Regression | Solubility |
| Drug Toxicity | `herg_karim.tab` | 5,000+ | Binary classification | HEPG2, Toxicity |
| Drug Half-Life | `half_life_obach.csv` | 600+ | Regression | Half-Life |
| ACE2 Binding | `ACE-2.csv` | 100+ | Classification | ACE2 |
| COX-2 Binding | `COX-2.csv` | 400+ | Classification | COX-2 |

**Data Sources:**
- ChEMBL (EMBL-EBI)
- PubChem (NIH)
- MoleculeNet (Stanford)
- Literature research papers

---

## ðŸ”§ For Contributors: Adding New Models

### 1. Add Your Dataset
```bash
mkdir -p backendML/New_Property/Datasets
# Place your .csv or .tab file here
```

### 2. Create a Training Notebook
```bash
# Create: backendML/New_Property/Notebook/my_model.ipynb
# Use existing notebooks as templates (e.g., BBBP.ipynb)
```

### 3. Update `train_all_models.py`
Add your model to the main training script:
```python
# In backendML/train_all_models.py
import your_notebook_module

def train_new_property():
    # Your training code
    model = ...
    joblib.dump(model, '../backend/models/new_property_model.pkl')
```

### 4. Update Backend Router
Create `../backend/routers/new_property.py`:
```python
@router.post("/predict/new-property")
def predict_new_property(payload: MoleculePayload):
    # Prediction logic
    pass
```

### 5. Test Locally
```bash
cd ../backend
python -m uvicorn main:app --reload --port 5001
# Test: curl http://localhost:5001/predict/new-property
```

---

## ðŸ§¬ Model Types & Algorithms

| Model | Algorithm | Type | Accuracy |
|-------|-----------|------|----------|
| **BBB Penetration** | Random Forest | Classification | 85-90% |
| **Solubility** | Gradient Boosting | Regression | MAE ~0.9 |
| **CYP3A4 Inhibition** | SVM | Classification | 80-85% |
| **Toxicity (HepG2)** | Neural Network | Classification | 90%+ |
| **ACE2 Binding** | Random Forest | Classification | 88%+ |
| **COX-2 Binding** | Gradient Boosting | Classification | 85%+ |
| **Half-Life** | Random Forest | Regression | MAE ~0.5 hrs |

**Feature Engineering:**
- RDKit molecular descriptors (200+ features)
- Fingerprints (ECFP, Morgan)
- Topological properties
- Electrostatic properties

---

## âš ï¸ Important Notes

### âœ… DO
- Use this folder for model research & development
- Retrain models with new datasets
- Experiment with hyperparameters in notebooks
- Cite data sources in your changes

### âŒ DON'T
- Treat this as the production API
- Forget to export trained models to `../backend/models/`
- Skip testing after retraining
- Commit large dataset files (>10MB) without `.gitignore`

---

## ðŸ”„ Workflow

### For Researchers
```
1. Explore data in Jupyter notebook
2. Train model with different hyperparameters
3. Evaluate performance (accuracy, MAE, etc.)
4. Export as .pkl file â†’ ../backend/models/
5. Test in FastAPI â†’ http://localhost:5001/predict/
```

### For Hackathon Competitors
```
1. The models are already trained (see ../backend/)
2. Use them via /predict/* endpoints
3. (If you want to improve: retrain here, then deploy to backend)
```

---

## ðŸ”— Related Files

- **Active Backend API:** [../backend/README.md](../backend/README.md)
- **Project Overview:** [../README.md](../README.md)
- **FastAPI Routes:** [../backend/routers/](../backend/routers/)
- **Model Loader:** [../backend/utils/model_loader.py](../backend/utils/model_loader.py)

---

## ðŸ“ž Questions?

- **"How do I retrain a model?"** â†’ Run `train_all_models.py` after updating datasets
- **"Can I add a new prediction model?"** â†’ Yes! Follow the "For Contributors" section above
- **"Where are the pre-trained models?"** â†’ [../backend/models/](../backend/models/)
- **"Does the API use updated models automatically?"** â†’ Yes, restart the API server

---

<div align="center">

**backendML** = The research & training lab  
**backend** = The production API lab  
Use the right tool for the right job! ðŸ§ªðŸš€

</div>
